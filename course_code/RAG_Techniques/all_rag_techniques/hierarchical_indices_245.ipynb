{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Indices in Document Retrieval\n",
    "\n",
    "## Overview\n",
    "\n",
    "This code implements a Hierarchical Indexing system for document retrieval, utilizing two levels of encoding: document-level summaries and detailed chunks. This approach aims to improve the efficiency and relevance of information retrieval by first identifying relevant document sections through summaries, then drilling down to specific details within those sections.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Traditional flat indexing methods can struggle with large documents or corpus, potentially missing context or returning irrelevant information. Hierarchical indexing addresses this by creating a two-tier search system, allowing for more efficient and context-aware retrieval.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "1. PDF processing and text chunking\n",
    "2. Asynchronous document summarization using OpenAI's GPT-4\n",
    "3. Vector store creation for both summaries and detailed chunks using FAISS and OpenAI embeddings\n",
    "4. Custom hierarchical retrieval function\n",
    "\n",
    "## Method Details\n",
    "\n",
    "### Document Preprocessing and Encoding\n",
    "\n",
    "1. The PDF is loaded and split into documents (likely by page).\n",
    "2. Each document is summarized asynchronously using GPT-4.\n",
    "3. The original documents are also split into smaller, detailed chunks.\n",
    "4. Two separate vector stores are created:\n",
    "   - One for document-level summaries\n",
    "   - One for detailed chunks\n",
    "\n",
    "### Asynchronous Processing and Rate Limiting\n",
    "\n",
    "1. The code uses asynchronous programming (asyncio) to improve efficiency.\n",
    "2. Implements batching and exponential backoff to handle API rate limits.\n",
    "\n",
    "### Hierarchical Retrieval\n",
    "\n",
    "The `retrieve_hierarchical` function implements the two-tier search:\n",
    "\n",
    "1. It first searches the summary vector store to identify relevant document sections.\n",
    "2. For each relevant summary, it then searches the detailed chunk vector store, filtering by the corresponding page number.\n",
    "3. This approach ensures that detailed information is retrieved only from the most relevant document sections.\n",
    "\n",
    "## Benefits of this Approach\n",
    "\n",
    "1. Improved Retrieval Efficiency: By first searching summaries, the system can quickly identify relevant document sections without processing all detailed chunks.\n",
    "2. Better Context Preservation: The hierarchical approach helps maintain the broader context of retrieved information.\n",
    "3. Scalability: This method is particularly beneficial for large documents or corpus, where flat searching might be inefficient or miss important context.\n",
    "4. Flexibility: The system allows for adjusting the number of summaries and chunks retrieved, enabling fine-tuning for different use cases.\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "1. Asynchronous Programming: Utilizes Python's asyncio for efficient I/O operations and API calls.\n",
    "2. Rate Limit Handling: Implements batching and exponential backoff to manage API rate limits effectively.\n",
    "3. Persistent Storage: Saves the generated vector stores locally to avoid unnecessary recomputation.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Hierarchical indexing represents a sophisticated approach to document retrieval, particularly suitable for large or complex document sets. By leveraging both high-level summaries and detailed chunks, it offers a balance between broad context understanding and specific information retrieval. This method has potential applications in various fields requiring efficient and context-aware information retrieval, such as legal document analysis, academic research, or large-scale content management systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"../images/hierarchical_indices.svg\" alt=\"hierarchical_indices\" style=\"width:50%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"../images/hierarchical_indices_example.svg\" alt=\"hierarchical_indices\" style=\"width:100%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "# from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.summarize.chain import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path sicnce we work with notebooks\n",
    "from helper_functions import *\n",
    "from evaluation.evalute_rag import *\n",
    "from helper_functions import encode_pdf, encode_from_string\n",
    "import bz2\n",
    "from loguru import logger\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "# load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define document path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_in_batches(dataset_path, batch_size, split=-1):\n",
    "    \"\"\"\n",
    "    Generator function that reads data from a compressed file and yields batches of data.\n",
    "    Each batch is a dictionary containing lists of interaction_ids, queries, search results, query times, and answers.\n",
    "\n",
    "    Args:\n",
    "    dataset_path (str): Path to the dataset file.\n",
    "    batch_size (int): Number of data items in each batch.\n",
    "\n",
    "    Yields:\n",
    "    dict: A batch of data.\n",
    "    \"\"\"\n",
    "\n",
    "    def initialize_batch():\n",
    "        \"\"\" Helper function to create an empty batch. \"\"\"\n",
    "        return {\"interaction_id\": [], \"query\": [], \"search_results\": [], \"query_time\": [], \"answer\": []}\n",
    "\n",
    "    try:\n",
    "        with bz2.open(dataset_path, \"rt\") as file:\n",
    "            batch = initialize_batch()\n",
    "            for line in file:\n",
    "                try:\n",
    "                    item = json.loads(line)\n",
    "\n",
    "                    if split != -1 and item[\"split\"] != split:\n",
    "                        continue\n",
    "\n",
    "                    for key in batch:\n",
    "                        batch[key].append(item[key])\n",
    "\n",
    "                    if len(batch[\"query\"]) == batch_size:\n",
    "                        yield batch\n",
    "                        batch = initialize_batch()\n",
    "                except json.JSONDecodeError:\n",
    "                    logger.warn(\"Warning: Failed to decode a line.\")\n",
    "            # Yield any remaining data as the last batch\n",
    "            if batch[\"query\"]:\n",
    "                yield batch\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Error: The file {dataset_path} was not found.\")\n",
    "        raise e\n",
    "    except IOError as e:\n",
    "        logger.error(f\"Error: An error occurred while reading the file {dataset_path}.\")\n",
    "        raise e\n",
    "def convert_html_to_text(html_source):\n",
    "        soup = BeautifulSoup(html_source, \"lxml\")\n",
    "        text = soup.get_text(\" \", strip=True)\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        return text\n",
    "async def batch_generate_db(batch, chunk_size=1000, chunk_overlap=200, is_string=False):\n",
    "    batch_interaction_ids = batch[\"interaction_id\"]\n",
    "    queries = batch[\"query\"]\n",
    "    batch_search_results = batch[\"search_results\"]\n",
    "    query_times = batch[\"query_time\"]\n",
    "    # import pdb; pdb.set_trace()\n",
    "    chunks = [[convert_html_to_text(html_source_i[\"page_result\"]) for html_source_i in html_source] for html_source in batch_search_results]\n",
    "    # Create document-level summaries\n",
    "    batch_size = 5  # Adjust this based on your rate limits\n",
    "    documents = []\n",
    "\n",
    "    for c_i,chunk in enumerate(chunks[0]):\n",
    "        doc =  Document(page_content=chunk, metadata={\"source\": \"local\", \"page\": c_i})\n",
    "        documents.append(doc)\n",
    "\n",
    "    summary_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=4000)\n",
    "    summary_chain = load_summarize_chain(summary_llm, chain_type=\"map_reduce\")\n",
    "    \n",
    "    async def summarize_doc(doc):\n",
    "        \"\"\"\n",
    "        Summarizes a single document with rate limit handling.\n",
    "        \n",
    "        Args:\n",
    "            doc: The document to be summarized.\n",
    "            \n",
    "        Returns:\n",
    "            A summarized Document object.\n",
    "        \"\"\"\n",
    "        # Retry the summarization with exponential backoff\n",
    "        summary_output = await retry_with_exponential_backoff(summary_chain.ainvoke([doc]))\n",
    "        summary = summary_output['output_text']\n",
    "        return Document(\n",
    "            page_content=summary,\n",
    "            metadata={\"source\": 'local', \"page\": doc.metadata[\"page\"], \"summary\": True}\n",
    "        )\n",
    "\n",
    "    # Process documents in smaller batches to avoid rate limits\n",
    "    \n",
    "    summaries = []\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        batch_summaries = await asyncio.gather(*[summarize_doc(doc) for doc in batch])\n",
    "        summaries.extend(batch_summaries)\n",
    "        await asyncio.sleep(1)  # Short pause between batches\n",
    "\n",
    "    # Split documents into detailed chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "    )\n",
    "    detailed_chunks = await asyncio.to_thread(text_splitter.split_documents, documents)\n",
    "\n",
    "    # Update metadata for detailed chunks\n",
    "    for i, chunk in enumerate(detailed_chunks):\n",
    "        chunk.metadata.update({\n",
    "            \"chunk_id\": i,\n",
    "            \"summary\": False,\n",
    "            \"page\": int(chunk.metadata.get(\"page\", 0))\n",
    "        })\n",
    "\n",
    "    # Create embeddings\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    # Create vector stores asynchronously with rate limit handling\n",
    "    async def create_vectorstore(docs):\n",
    "        \"\"\"\n",
    "        Creates a vector store from a list of documents with rate limit handling.\n",
    "        \n",
    "        Args:\n",
    "            docs: The list of documents to be embedded.\n",
    "            \n",
    "        Returns:\n",
    "            A FAISS vector store containing the embedded documents.\n",
    "        \"\"\"\n",
    "        return await retry_with_exponential_backoff(\n",
    "            asyncio.to_thread(FAISS.from_documents, docs, embeddings)\n",
    "        )\n",
    "\n",
    "    # Generate vector stores for summaries and detailed chunks concurrently\n",
    "    summary_vectorstore, detailed_vectorstore = await asyncio.gather(\n",
    "        create_vectorstore(summaries),\n",
    "        create_vectorstore(detailed_chunks)\n",
    "    )\n",
    "\n",
    "    return summary_vectorstore, detailed_vectorstore\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_hierarchical(query, summary_vectorstore, detailed_vectorstore, k_summaries=3, k_chunks=5):\n",
    "    \"\"\"\n",
    "    Performs a hierarchical retrieval using the query.\n",
    "\n",
    "    Args:\n",
    "        query: The search query.\n",
    "        summary_vectorstore: The vector store containing document summaries.\n",
    "        detailed_vectorstore: The vectaor store containing detailed chunks.\n",
    "        k_summaries: The number of top summaries to retrieve.\n",
    "        k_chunks: The number of detailed chunks to retrieve per summary.\n",
    "\n",
    "    Returns:\n",
    "        A list of relevant detailed chunks.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve top summaries\n",
    "    top_summaries = summary_vectorstore.similarity_search(query, k=k_summaries)\n",
    "    \n",
    "    relevant_chunks = []\n",
    "    for summary in top_summaries:\n",
    "        # For each summary, retrieve relevant detailed chunks\n",
    "        page_number = summary.metadata[\"page\"]\n",
    "        page_filter = lambda metadata: metadata[\"page\"] == page_number\n",
    "        page_chunks = detailed_vectorstore.similarity_search(\n",
    "            query, \n",
    "            k=k_chunks, \n",
    "            filter=page_filter\n",
    "        )\n",
    "        relevant_chunks.extend(page_chunks)\n",
    "    \n",
    "    return relevant_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to encode to both summary and chunk levels, sharing the page metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_47484/1014682191.py\u001b[0m(1)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m----> 1 \u001b[0;31m\u001b[0;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_data_in_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/root/crag/crag_task_1_dev_v4_release.jsonl.bz2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      2 \u001b[0;31m        \u001b[0mbatch_ground_truths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"answer\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Remove answers from batch and store them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      3 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      4 \u001b[0;31m            \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      5 \u001b[0;31m        \u001b[0msummary_store\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetailed_store\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mbatch_generate_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/tmp/ipykernel_47484/1014682191.py\u001b[0m(1)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m----> 1 \u001b[0;31m\u001b[0;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_data_in_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/root/crag/crag_task_1_dev_v4_release.jsonl.bz2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      2 \u001b[0;31m        \u001b[0mbatch_ground_truths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"answer\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Remove answers from batch and store them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      3 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      4 \u001b[0;31m            \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      5 \u001b[0;31m        \u001b[0msummary_store\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetailed_store\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mbatch_generate_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "[Document(metadata={'source': 'local', 'page': 1, 'chunk_id': 45, 'summary': False}, page_content='development roles. [3] At 23, he was named Oracle\\'s Rookie of the Year. Three years later, he became the youngest person in the company\\'s history to earn the title of vice president. [3] Benioff founded Salesforce in 1999 [19] in a San Francisco apartment and defined its mission in a marketing statement as \"The End of Software.\" [20] This was a slogan he used frequently to preach about software on the Web, and used as a guerilla marketing tactic against the dominant CD-ROM CRM competitor Siebel at the time. [21] Benioff extended Salesforce\\'s offerings in the early 2000s with the idea of a platform that allowed developers to create applications. [22] Salesforce is now the biggest employer in San Francisco and the anchor tenant of Salesforce Tower , the tallest building in San Francisco. [23] Benioff also serves on the World Economic Forum\\'s board of trustees and USC board of trustees. [3] [7] On September 16, 2018, Marc and his wife Lynne bought Time for $190 million. [4] In 2019,'), Document(metadata={'source': 'local', 'page': 1, 'chunk_id': 43, 'summary': False}, page_content='and philanthropist. Benioff is best known as the co-founder, chairman and CEO of the software company Salesforce , as well as being the owner of Time magazine since 2018. [3] [4] Early life and education [ edit ] Benioff was raised in a Jewish family [5] [6] in the San Francisco Bay Area . [7] He is the grandson of Marvin Lewis, a California trial attorney and once-time member of the San Francisco Board of Supervisors who championed the creation of the BART system. [8] [9] Benioff grew up in Hillsborough [10] and graduated from Burlingame High School in 1982. [11] Benioff received a Bachelor of Science in business administration from the University of Southern California , where he was a member of the Tau Kappa Epsilon fraternity , in 1986. [11] [12] Benioff is a second cousin of showrunner and television writer David Benioff , known for Game of Thrones . [13] He is married to Lynne Benioff and has two children. The family lives in San Francisco, California. [3] [11] Career [ edit ]'), Document(metadata={'source': 'local', 'page': 1, 'chunk_id': 48, 'summary': False}, page_content='with Carlye Adler. [38] In 2009, he co-wrote Behind the Cloud: The Untold Story of How Salesforce.com Went from Idea to Billion-Dollar Company and Revolutionized an Industry , also with Carlye Adler. [39] In 2019, he again co-wrote Trailblazer: The Power of Business as the Greatest Platform for Change , with Monica Langley . [38] The book became a New York Times bestseller . [40] Recognition [ edit ] Benioff during the WEF 2013 In 2003, President George W. Bush appointed Benioff co-chair of the President\\'s Information Technology Advisory Committee. [41] In 2009, Benioff was named a Young Global Leader by the World Economic Forum , and is a member of its board of trustees. [42] [43] In 2012, he was named one of the \"Best CEOs in the World\" by Barron \\' s [44] and received The Economist \\' s Innovation Award. [45] In 2014, Fortune readers voted him \"Businessperson of the Year.\" [46] In 2016, Fortune named him one of the \"World\\'s 50 Greatest Leaders.\" [47] In 2019, he was recognized as one'), Document(metadata={'source': 'local', 'page': 3, 'chunk_id': 84, 'summary': False}, page_content='former Slack CEO Stewart Butterfield also announced their departures. When asked about the departures, Benioff stated, \"people come and people go\"; Salesforce\\'s stock dropped to a 52-week low after Nelson\\'s resignation. [33] [34] [35] [36] In January 2023, the company announced a layoff of about 10% or approximately 8,000 positions. According to Benioff, the company hired too aggressively during the COVID-19 pandemic and the increase in working from home led to the layoff. The company will also reduce office space as part of the restructuring plan. [37] The same month brought an announcement from activist investor Elliott Management that it would acquire a \"big stake\" in the company. [38] In January 2024, Salesforce announced it was laying off 700 employees (about 1%) of its global staff. [39] Services [ edit ] Salesforce offers several customer relationship management (CRM) services, including: Sales Cloud, [40] Service Cloud, [41] Marketing Cloud , [42] and Commerce Cloud and'), Document(metadata={'source': 'local', 'page': 3, 'chunk_id': 83, 'summary': False}, page_content='in Salesforce\\'s earnings call, where he stated he looked at \"this quarter very much as kind of a milestone\". [3] Salesforce announced a partnership with Meta Platforms in September 2022. The deal called for Meta\\'s consumer application WhatsApp to integrate Salesforce\\'s Customer 360 platform to allow consumers to communicate with companies directly. [30] In November 2022, Salesforce announced it would terminate employees in its sales organization. [31] Protocol reported that the company would likely eliminate some 2500 jobs. [32] In November 2022, Salesforce announced its co-CEO and vice chair, Bret Taylor, would be stepping down from his roles at the end of January 2023, with Benioff continuing to run the company and serve as board chair. Within the week, former Tableau CEO Mark Nelson and former Slack CEO Stewart Butterfield also announced their departures. When asked about the departures, Benioff stated, \"people come and people go\"; Salesforce\\'s stock dropped to a 52-week low after'), Document(metadata={'source': 'local', 'page': 3, 'chunk_id': 81, 'summary': False}, page_content=\"the Dow Jones Industrial Average , replacing energy giant and Standard Oil -descendant ExxonMobil . [19] Salesforce's ascension to the Dow Jones was concurrent with that of Amgen and Honeywell . [5] Because the Dow Jones factors its components by market price, Salesforce was the largest technology component of the index at its accession. [20] Across 2020 and 2021, Salesforce saw some notable leadership changes; in February 2020, co-chief executive officer Keith Block stepped down from his position in the company. [21] Marc Benioff remained as chairman and chief executive officer. [22] In February 2021, Amy Weaver, previously the chief legal officer, became CFO. Former CFO Mark Hawkins announced that he would be retiring in October. [23] [24] In November 2021, Bret Taylor was named vice chair and co-CEO of the company. [25] In December 2020, it was announced that Salesforce would acquire Slack for $27.7 billion, its largest acquisition to date. [26] The acquisition closed in July 2021.\"), Document(metadata={'source': 'local', 'page': 3, 'chunk_id': 79, 'summary': False}, page_content='the founder of CNET . [7] Salesforce was severely affected by the dot-com bubble bursting at the beginning of the new millennium, with the company laying off 20% of its workforce. Despite its losses, Salesforce continued strong during the early 2000s. Salesforce also gained notability during this period for its \"the end of software\" tagline and marketing campaign, in which it also hired actors to hold up signs with its slogan outside a Siebel Systems conference. [8] Salesforce\\'s revenue continued to increase from 2000 to 2003, with 2003\\'s revenue skyrocketing from $5.4 million in the fiscal year 2001 to over $100 million by December 2003. [9] Also in 2003, Salesforce held its first annual Dreamforce conference in San Francisco. [10] In June 2004, the company had its initial public offering on the New York Stock Exchange under the stock symbol CRM and raised US$110\\xa0million. [11] [12] In 2006, Salesforce launched IdeaExchange, a platform that allows customers to connect with company'), Document(metadata={'source': 'local', 'page': 3, 'chunk_id': 76, 'summary': False}, page_content='Upload file Special pages Permanent link Page information Cite this page Get shortened URL Download QR code Wikidata item Print/export Download as PDF Printable version In other projects Wikimedia Commons From Wikipedia, the free encyclopedia American software company Salesforce, Inc. Salesforce Tower in San Francisco, the headquarters of Salesforce Formerly Salesforce.com, Inc. (1999â€“2022) Company type Public Traded as NYSE : CRM DJIA component S&P 100 component S&P 500 component Industry Cloud computing Enterprise software Consulting Founded February\\xa03, 1999 ; 25 years ago ( 1999-02-03 ) Founders Marc Benioff Parker Harris Dave Moellenhoff Frank Dominguez Headquarters Salesforce Tower San Francisco, California , U.S. Key people Marc Benioff ( Chairman & CEO) Services Cloud computing Revenue US$ 31.35 billion (2023) Operating income US$1.03 billion (2023) Net income US$208 million (2023) Total assets US$98.85 billion (2023) Total equity US$58.36 billion (2023) Number of employees'), Document(metadata={'source': 'local', 'page': 2, 'chunk_id': 72, 'summary': False}, page_content='Kendall Collins Chief Business Officer & Chief of Staff to Marc Benioff Salesforce View bio Miguel Milano President & Chief Revenue Officer Salesforce View bio Nathalie Scardino Chief People Officer Salesforce View bio Sabastian Niles President & Chief Legal Officer Salesforce View bio Srini Tallapragada President & Chief Engineering Officer Salesforce View bio Suzanne DiBianca Executive Vice President & Chief Impact Officer Salesforce View bio Board of Directors Marc Benioff Chair, CEO & Co-Founder Salesforce View bio Parker Harris Co-Founder, Salesforce & Chief Technology Officer, Slack Salesforce View bio Laura Alber President & CEO Williams-Sonoma, Inc. View bio Craig Conway Former President & CEO PeopleSoft, Inc. View bio Arnold Donald Former President & CEO Carnival Corporation & plc View bio Neelie Kroes Former Vice President European Commission View bio Sachin Mehra Financial Expert CFO Mastercard View bio Mason Morfit CEO & Chief Investment Officer ValueAct Capital View bio'), Document(metadata={'source': 'local', 'page': 2, 'chunk_id': 71, 'summary': False}, page_content='Leadership - Salesforce.com US Skip to content Salesforce Leadership Salesforce prides itself not only on award-winning technology, but also on the talent of its people. The company is thriving under the guidance and leadership of some of the brightest minds and most experienced executives in business. Executive Team Marc Benioff Chair, CEO & Co-Founder Salesforce View bio Parker Harris Co-Founder, Salesforce & Chief Technology Officer, Slack Salesforce View bio Amy Weaver President & Chief Financial Officer Salesforce View bio Ariel Kelman President & Chief Marketing Officer Salesforce View bio Brian Millham President & Chief Operating Officer Salesforce View bio David Schmaier President & Chief Product Officer Salesforce View bio Juan Perez Chief Information Officer Salesforce View bio Kendall Collins Chief Business Officer & Chief of Staff to Marc Benioff Salesforce View bio Miguel Milano President & Chief Revenue Officer Salesforce View bio Nathalie Scardino Chief People Officer')]\n",
      "10\n",
      "'where did the ceo of salesforce previously work?'\n",
      "'where did the ceo of salesforce previously work?'\n"
     ]
    }
   ],
   "source": [
    "for batch_index, batch in enumerate(load_data_in_batches('/root/crag/crag_task_1_dev_v4_release.jsonl.bz2', 1, -1)):\n",
    "        batch_ground_truths = batch.pop(\"answer\")  # Remove answers from batch and store them\n",
    "        if batch_index<1:\n",
    "            continue\n",
    "        summary_store, detailed_store  = await batch_generate_db(batch)\n",
    "        summary_store.save_local(\"../vector_stores/summary_store\")\n",
    "        detailed_store.save_local(\"../vector_stores/detailed_store\")\n",
    "        query = batch[\"query\"][0]\n",
    "        relavant_truck = retrieve_hierarchical(query, summary_store, detailed_store)\n",
    "        # import pdb; pdb.set_trace()\n",
    "        # # Print results\n",
    "        # for chunk in relavant_truck:\n",
    "        #     # print(f\"Page: {chunk.metadata['page']}\")\n",
    "        #     print(f\"Content: {chunk.page_content}...\")  # Print first 100 characters\n",
    "        #     print(\"---\")\n",
    "        # queries.extend(batch[\"query\"])\n",
    "        # ground_truths.extend(batch_ground_truths)\n",
    "        # predictions.extend(batch_predictions)\n",
    "\n",
    "        # # Save the entire batch as a JSON file\n",
    "        # batch_data = {\n",
    "        #     \"queries\": batch[\"query\"],\n",
    "        #     \"ground_truths\": batch_ground_truths,\n",
    "        #     \"predictions\": batch_predictions\n",
    "        # }\n",
    "        # file_path = os.path.join(output_directory, f\"batch_prediction_{batch_index}.json\")\n",
    "        # with open(file_path, \"w\") as f:\n",
    "        #     json.dump(batch_data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def encode_pdf_hierarchical(path, chunk_size=1000, chunk_overlap=200, is_string=False):\n",
    "    \"\"\"\n",
    "    Asynchronously encodes a PDF book into a hierarchical vector store using OpenAI embeddings.\n",
    "    Includes rate limit handling with exponential backoff.\n",
    "    \n",
    "    Args:\n",
    "        path: The path to the PDF file.\n",
    "        chunk_size: The desired size of each text chunk.\n",
    "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple containing two FAISS vector stores:\n",
    "        1. Document-level summaries\n",
    "        2. Detailed chunks\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load PDF documents\n",
    "    if not is_string:\n",
    "        loader = PyPDFLoader(path)\n",
    "        documents = await asyncio.to_thread(loader.load)\n",
    "    else:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            # Set a really small chunk size, just to show.\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False,\n",
    "        )\n",
    "        documents = text_splitter.create_documents([path])\n",
    "        import pdb; pdb.set_trace()\n",
    "    # Create document-level summaries\n",
    "    summary_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=4000)\n",
    "    summary_chain = load_summarize_chain(summary_llm, chain_type=\"map_reduce\")\n",
    "    \n",
    "    async def summarize_doc(doc):\n",
    "        \"\"\"\n",
    "        Summarizes a single document with rate limit handling.\n",
    "        \n",
    "        Args:\n",
    "            doc: The document to be summarized.\n",
    "            \n",
    "        Returns:\n",
    "            A summarized Document object.\n",
    "        \"\"\"\n",
    "        # Retry the summarization with exponential backoff\n",
    "        summary_output = await retry_with_exponential_backoff(summary_chain.ainvoke([doc]))\n",
    "        summary = summary_output['output_text']\n",
    "        return Document(\n",
    "            page_content=summary,\n",
    "            metadata={\"source\": path, \"page\": doc.metadata[\"page\"], \"summary\": True}\n",
    "        )\n",
    "\n",
    "    # Process documents in smaller batches to avoid rate limits\n",
    "    batch_size = 5  # Adjust this based on your rate limits\n",
    "    summaries = []\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        batch_summaries = await asyncio.gather(*[summarize_doc(doc) for doc in batch])\n",
    "        summaries.extend(batch_summaries)\n",
    "        await asyncio.sleep(1)  # Short pause between batches\n",
    "\n",
    "    # Split documents into detailed chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "    )\n",
    "    detailed_chunks = await asyncio.to_thread(text_splitter.split_documents, documents)\n",
    "\n",
    "    # Update metadata for detailed chunks\n",
    "    for i, chunk in enumerate(detailed_chunks):\n",
    "        chunk.metadata.update({\n",
    "            \"chunk_id\": i,\n",
    "            \"summary\": False,\n",
    "            \"page\": int(chunk.metadata.get(\"page\", 0))\n",
    "        })\n",
    "\n",
    "    # Create embeddings\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    # Create vector stores asynchronously with rate limit handling\n",
    "    async def create_vectorstore(docs):\n",
    "        \"\"\"\n",
    "        Creates a vector store from a list of documents with rate limit handling.\n",
    "        \n",
    "        Args:\n",
    "            docs: The list of documents to be embedded.\n",
    "            \n",
    "        Returns:\n",
    "            A FAISS vector store containing the embedded documents.\n",
    "        \"\"\"\n",
    "        return await retry_with_exponential_backoff(\n",
    "            asyncio.to_thread(FAISS.from_documents, docs, embeddings)\n",
    "        )\n",
    "\n",
    "    # Generate vector stores for summaries and detailed chunks concurrently\n",
    "    summary_vectorstore, detailed_vectorstore = await asyncio.gather(\n",
    "        create_vectorstore(summaries),\n",
    "        create_vectorstore(detailed_chunks)\n",
    "    )\n",
    "\n",
    "    return summary_vectorstore, detailed_vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the PDF book to both document-level summaries and detailed chunks if the vector stores do not exist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/openai/_base_client.py:1549\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1549\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[1;32m   1550\u001b[0m         request,\n\u001b[1;32m   1551\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[1;32m   1552\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1553\u001b[0m     )\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpx/_client.py:1674\u001b[0m, in \u001b[0;36mAsyncClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m   1672\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m-> 1674\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[1;32m   1675\u001b[0m     request,\n\u001b[1;32m   1676\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m   1677\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1678\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m   1679\u001b[0m )\n\u001b[1;32m   1680\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpx/_client.py:1702\u001b[0m, in \u001b[0;36mAsyncClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1701\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1702\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[1;32m   1703\u001b[0m         request,\n\u001b[1;32m   1704\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1705\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m   1706\u001b[0m     )\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpx/_client.py:1739\u001b[0m, in \u001b[0;36mAsyncClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[0;32m-> 1739\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[1;32m   1740\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpx/_client.py:1776\u001b[0m, in \u001b[0;36mAsyncClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1776\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m transport\u001b[38;5;241m.\u001b[39mhandle_async_request(request)\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, AsyncByteStream)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpx/_transports/default.py:377\u001b[0m, in \u001b[0;36mAsyncHTTPTransport.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 377\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_async_request(req)\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mAsyncIterable)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpcore/_async/connection_pool.py:216\u001b[0m, in \u001b[0;36mAsyncConnectionPool.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpcore/_async/connection_pool.py:196\u001b[0m, in \u001b[0;36mAsyncConnectionPool.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m connection\u001b[38;5;241m.\u001b[39mhandle_async_request(\n\u001b[1;32m    197\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpcore/_async/connection.py:101\u001b[0m, in \u001b[0;36mAsyncHTTPConnection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_async_request(request)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpcore/_async/http11.py:143\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpcore/_async/http11.py:113\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpcore/_async/http11.py:186\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpcore/_async/http11.py:224\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    226\u001b[0m     )\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpcore/_backends/anyio.py:35\u001b[0m, in \u001b[0;36mAnyIOStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream\u001b[38;5;241m.\u001b[39mreceive(max_bytes\u001b[38;5;241m=\u001b[39mmax_bytes)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39mEndOfStream:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/anyio/streams/tls.py:205\u001b[0m, in \u001b[0;36mTLSStream.receive\u001b[0;34m(self, max_bytes)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreceive\u001b[39m(\u001b[38;5;28mself\u001b[39m, max_bytes: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m65536\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbytes\u001b[39m:\n\u001b[0;32m--> 205\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_sslobject_method(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ssl_object\u001b[38;5;241m.\u001b[39mread, max_bytes)\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/anyio/streams/tls.py:147\u001b[0m, in \u001b[0;36mTLSStream._call_sslobject_method\u001b[0;34m(self, func, *args)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransport_stream\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_bio\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m--> 147\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransport_stream\u001b[38;5;241m.\u001b[39mreceive()\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EndOfStream:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/anyio/_backends/_asyncio.py:1142\u001b[0m, in \u001b[0;36mSocketStream.receive\u001b[0;34m(self, max_bytes)\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mresume_reading()\n\u001b[0;32m-> 1142\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol\u001b[38;5;241m.\u001b[39mread_event\u001b[38;5;241m.\u001b[39mwait()\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mpause_reading()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/asyncio/locks.py:214\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/RAG_Techniques/helper_functions.py:311\u001b[0m, in \u001b[0;36mretry_with_exponential_backoff\u001b[0;34m(coroutine, max_retries)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# Attempt to execute the coroutine\u001b[39;00m\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m coroutine\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RateLimitError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# If the last attempt also fails, raise the exception\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:215\u001b[0m, in \u001b[0;36mChain.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:206\u001b[0m, in \u001b[0;36mChain.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    205\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acall(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acall(inputs)\n\u001b[1;32m    209\u001b[0m )\n\u001b[1;32m    210\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maprep_outputs(\n\u001b[1;32m    211\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    212\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py:154\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._acall\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    153\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[0;32m--> 154\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macombine_docs(\n\u001b[1;32m    155\u001b[0m     docs, callbacks\u001b[38;5;241m=\u001b[39m_run_manager\u001b[38;5;241m.\u001b[39mget_child(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mother_keys\n\u001b[1;32m    156\u001b[0m )\n\u001b[1;32m    157\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/combine_documents/map_reduce.py:268\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain.acombine_docs\u001b[0;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m result_docs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    264\u001b[0m     Document(page_content\u001b[38;5;241m=\u001b[39mr[question_result_key], metadata\u001b[38;5;241m=\u001b[39mdocs[i]\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;66;03m# This uses metadata from the docs, and the textual results from `results`\u001b[39;00m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(map_results)\n\u001b[1;32m    267\u001b[0m ]\n\u001b[0;32m--> 268\u001b[0m result, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce_documents_chain\u001b[38;5;241m.\u001b[39macombine_docs(\n\u001b[1;32m    269\u001b[0m     result_docs, token_max\u001b[38;5;241m=\u001b[39mtoken_max, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    270\u001b[0m )\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_intermediate_steps:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/combine_documents/reduce.py:272\u001b[0m, in \u001b[0;36mReduceDocumentsChain.acombine_docs\u001b[0;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m result_docs, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acollapse(\n\u001b[1;32m    270\u001b[0m     docs, token_max\u001b[38;5;241m=\u001b[39mtoken_max, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    271\u001b[0m )\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_documents_chain\u001b[38;5;241m.\u001b[39macombine_docs(\n\u001b[1;32m    273\u001b[0m     docs\u001b[38;5;241m=\u001b[39mresult_docs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    274\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/combine_documents/stuff.py:275\u001b[0m, in \u001b[0;36mStuffDocumentsChain.acombine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# Call predict on the LLM.\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mapredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs), {}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:333\u001b[0m, in \u001b[0;36mLLMChain.apredict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03m        completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macall(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks))[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:189\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.awarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m     emit_warning()\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:431\u001b[0m, in \u001b[0;36mChain.acall\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    425\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    430\u001b[0m }\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[1;32m    432\u001b[0m     inputs,\n\u001b[1;32m    433\u001b[0m     cast(RunnableConfig, {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}),\n\u001b[1;32m    434\u001b[0m     return_only_outputs\u001b[38;5;241m=\u001b[39mreturn_only_outputs,\n\u001b[1;32m    435\u001b[0m     include_run_info\u001b[38;5;241m=\u001b[39minclude_run_info,\n\u001b[1;32m    436\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:215\u001b[0m, in \u001b[0;36mChain.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:206\u001b[0m, in \u001b[0;36mChain.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    205\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acall(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acall(inputs)\n\u001b[1;32m    209\u001b[0m )\n\u001b[1;32m    210\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maprep_outputs(\n\u001b[1;32m    211\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    212\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:298\u001b[0m, in \u001b[0;36mLLMChain._acall\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acall\u001b[39m(\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    295\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    296\u001b[0m     run_manager: Optional[AsyncCallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    297\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 298\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate([inputs], run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py:165\u001b[0m, in \u001b[0;36mLLMChain.agenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39magenerate_prompt(\n\u001b[1;32m    166\u001b[0m         prompts,\n\u001b[1;32m    167\u001b[0m         stop,\n\u001b[1;32m    168\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs,\n\u001b[1;32m    170\u001b[0m     )\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:787\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    786\u001b[0m prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 787\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate(\n\u001b[1;32m    788\u001b[0m     prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    789\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:713\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m run_managers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mon_chat_model_start(\n\u001b[1;32m    704\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    705\u001b[0m     messages,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    710\u001b[0m     run_id\u001b[38;5;241m=\u001b[39mrun_id,\n\u001b[1;32m    711\u001b[0m )\n\u001b[0;32m--> 713\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;241m*\u001b[39m[\n\u001b[1;32m    715\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate_with_cache(\n\u001b[1;32m    716\u001b[0m             m,\n\u001b[1;32m    717\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    718\u001b[0m             run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    719\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    720\u001b[0m         )\n\u001b[1;32m    721\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages)\n\u001b[1;32m    722\u001b[0m     ],\n\u001b[1;32m    723\u001b[0m     return_exceptions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    724\u001b[0m )\n\u001b[1;32m    725\u001b[0m exceptions \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# if os.path.exists(\"../vector_stores/summary_store\") and os.path.exists(\"../vector_stores/detailed_store\"):\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#    embeddings = OpenAIEmbeddings()\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#    summary_store = FAISS.load_local(\"../vector_stores/summary_store\", embeddings, allow_dangerous_deserialization=True)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#    detailed_store = FAISS.load_local(\"../vector_stores/detailed_store\", embeddings, allow_dangerous_deserialization=True)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m summary_store, detailed_store \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m encode_pdf_hierarchical(path)\n\u001b[1;32m      8\u001b[0m summary_store\u001b[38;5;241m.\u001b[39msave_local(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../vector_stores/summary_store\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m detailed_store\u001b[38;5;241m.\u001b[39msave_local(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../vector_stores/detailed_store\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 59\u001b[0m, in \u001b[0;36mencode_pdf_hierarchical\u001b[0;34m(path, chunk_size, chunk_overlap, is_string)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(documents), batch_size):\n\u001b[1;32m     58\u001b[0m     batch \u001b[38;5;241m=\u001b[39m documents[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[0;32m---> 59\u001b[0m     batch_summaries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39m[summarize_doc(doc) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m batch])\n\u001b[1;32m     60\u001b[0m     summaries\u001b[38;5;241m.\u001b[39mextend(batch_summaries)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Short pause between batches\u001b[39;00m\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# if os.path.exists(\"../vector_stores/summary_store\") and os.path.exists(\"../vector_stores/detailed_store\"):\n",
    "#    embeddings = OpenAIEmbeddings()\n",
    "#    summary_store = FAISS.load_local(\"../vector_stores/summary_store\", embeddings, allow_dangerous_deserialization=True)\n",
    "#    detailed_store = FAISS.load_local(\"../vector_stores/detailed_store\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# else:\n",
    "summary_store, detailed_store = await encode_pdf_hierarchical(path)\n",
    "summary_store.save_local(\"../vector_stores/summary_store\")\n",
    "detailed_store.save_local(\"../vector_stores/detailed_store\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve information according to summary level, and then retrieve information from the chunk level vector store and filter according to the summary level pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_hierarchical(query, summary_vectorstore, detailed_vectorstore, k_summaries=3, k_chunks=5):\n",
    "    \"\"\"\n",
    "    Performs a hierarchical retrieval using the query.\n",
    "\n",
    "    Args:\n",
    "        query: The search query.\n",
    "        summary_vectorstore: The vector store containing document summaries.\n",
    "        detailed_vectorstore: The vectaor store containing detailed chunks.\n",
    "        k_summaries: The number of top summaries to retrieve.\n",
    "        k_chunks: The number of detailed chunks to retrieve per summary.\n",
    "\n",
    "    Returns:\n",
    "        A list of relevant detailed chunks.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve top summaries\n",
    "    top_summaries = summary_vectorstore.similarity_search(query, k=k_summaries)\n",
    "    \n",
    "    relevant_chunks = []\n",
    "    for summary in top_summaries:\n",
    "        # For each summary, retrieve relevant detailed chunks\n",
    "        page_number = summary.metadata[\"page\"]\n",
    "        page_filter = lambda metadata: metadata[\"page\"] == page_number\n",
    "        page_chunks = detailed_vectorstore.similarity_search(\n",
    "            query, \n",
    "            k=k_chunks, \n",
    "            filter=page_filter\n",
    "        )\n",
    "        relevant_chunks.extend(page_chunks)\n",
    "    \n",
    "    return relevant_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate on a use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 0\n",
      "Content: driven by human activities, particularly the emission of greenhou se gases.  \n",
      "Chapter 2: Causes of Climate Change  \n",
      "Greenhouse Gases  \n",
      "The primary cause of recent climate change is the increase in greenhouse gases in the \n",
      "atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous \n",
      "oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is  essential \n",
      "for life on Earth, as it keeps the planet warm enough to support life. However, human \n",
      "activities have intensified this natural process, leading to a warmer climate.  \n",
      "Fossil Fuels  \n",
      "Burning fossil fuels for energy releases large amounts of CO2. This includes coal, oil, and \n",
      "natural gas used for electricity, heating, and transportation. The industrial revolution marked \n",
      "the beginning of a significant increase in fossil fuel consumption, which continues to rise \n",
      "today.  \n",
      "Coal...\n",
      "---\n",
      "Page: 0\n",
      "Content: Most of these climate changes are attributed to very small variations in Earth's orbit that \n",
      "change the amount of solar energy our planet receives. During the Holocene epoch, which \n",
      "began at the end of the last ice age, human societies f lourished, but the industrial era has seen \n",
      "unprecedented changes.  \n",
      "Modern Observations  \n",
      "Modern scientific observations indicate a rapid increase in global temperatures, sea levels, \n",
      "and extreme weather events. The Intergovernmental Panel on Climate Change (IPCC) has \n",
      "documented these changes extensively. Ice core samples, tree rings, and ocean sediments \n",
      "provide a historical record that scientists use to understand past climate conditions and \n",
      "predict future trends. The evidence overwhelmingly shows that recent changes are primarily \n",
      "driven by human activities, particularly the emission of greenhou se gases.  \n",
      "Chapter 2: Causes of Climate Change  \n",
      "Greenhouse Gases...\n",
      "---\n",
      "Page: 0\n",
      "Content: Understanding Climate Change  \n",
      "Chapter 1: Introduction to Climate Change  \n",
      "Climate change refers to significant, long -term changes in the global climate. The term \n",
      "\"global climate\" encompasses the planet's overall weather patterns, including temperature, \n",
      "precipitation, and wind patterns, over an extended period. Over the past cent ury, human \n",
      "activities, particularly the burning of fossil fuels and deforestation, have significantly \n",
      "contributed to climate change.  \n",
      "Historical Context  \n",
      "The Earth's climate has changed throughout history. Over the past 650,000 years, there have \n",
      "been seven cycles of glacial advance and retreat, with the abrupt end of the last ice age about \n",
      "11,700 years ago marking the beginning of the modern climate era and  human civilization. \n",
      "Most of these climate changes are attributed to very small variations in Earth's orbit that \n",
      "change the amount of solar energy our planet receives. During the Holocene epoch, which...\n",
      "---\n",
      "Page: 2\n",
      "Content: development of eco -friendly fertilizers and farming techniques is  essential for reducing the \n",
      "agricultural sector's carbon footprint.  \n",
      "Chapter 3: Effects of Climate Change  \n",
      "The effects of climate change are already being felt around the world and are projected to \n",
      "intensify in the coming decades. These effects include:  \n",
      "Rising Temperatures  \n",
      "Global temperatures have risen by about 1.2 degrees Celsius (2.2 degrees Fahrenheit) since \n",
      "the late 19th century. This warming is not uniform, with some regions experiencing more \n",
      "significant increases than others.  \n",
      "Heatwaves  \n",
      "Heatwaves are becoming more frequent and severe, posing risks to human health, agriculture, \n",
      "and infrastructure. Cities are particularly vulnerable due to the \"urban heat island\" effect. \n",
      "Heatwaves can lead to heat -related illnesses and exacerbate existing h ealth conditions.  \n",
      "Changing Seasons  \n",
      "Climate change is altering the timing and length of seasons, affecting ecosystems and human...\n",
      "---\n",
      "Page: 2\n",
      "Content: Changing Seasons  \n",
      "Climate change is altering the timing and length of seasons, affecting ecosystems and human \n",
      "activities. For example, spring is arriving earlier, and winters are becoming shorter and \n",
      "milder in many regions. This shift disrupts plant and animal life cycles a nd agricultural \n",
      "practices.  \n",
      "Melting Ice and Rising Sea Levels  \n",
      "Warmer temperatures are causing polar ice caps and glaciers to melt, contributing to rising \n",
      "sea levels. Sea levels have risen by about 20 centimeters (8 inches) in the past century, \n",
      "threatening coastal communities and ecosystems.  \n",
      "Polar Ice Melt...\n",
      "---\n",
      "Page: 2\n",
      "Content: Ruminant animals, such as cows and sheep, produce methane during digestion. Manure \n",
      "management practices also contribute to methane and nitrous oxide emissions. Innovations in \n",
      "livestock feeding and waste management can help mitigate these emissions.  \n",
      "Rice Cultivation  \n",
      "Flooded rice paddies create anaerobic conditions that lead to methane production. Improved \n",
      "water management and rice varieties can help reduce these emissions. Research into \n",
      "sustainable rice farming practices is crucial for balancing food security and clim ate goals.  \n",
      "Fertilizers  \n",
      "The use of synthetic fertilizers in agriculture releases nitrous oxide, a potent greenhouse gas. \n",
      "Practices such as precision farming and organic fertilizers can mitigate these emissions. The \n",
      "development of eco -friendly fertilizers and farming techniques is  essential for reducing the \n",
      "agricultural sector's carbon footprint.  \n",
      "Chapter 3: Effects of Climate Change...\n",
      "---\n",
      "Page: 5\n",
      "Content: Energy -efficient buildings use less energy for heating, cooling, and lighting. This can be \n",
      "achieved through better insulation, energy -efficient windows, and smart building \n",
      "technologies. Retrofitting existing buildings is also crucial for enhancing efficien cy. \n",
      "Transportation Efficiency  \n",
      "Improving the fuel efficiency of vehicles, promoting public transportation, and supporting \n",
      "electric vehicles can reduce emissions from the transportation sector. Investments in \n",
      "sustainable transport infrastructure are vital for reducing carbon footprints.  \n",
      "Industrial Efficiency  \n",
      "Enhancing the energy efficiency of industrial processes and equipment can reduce emissions \n",
      "and lower operating costs. This includes upgrading machinery, optimizing production \n",
      "processes, and recovering waste heat. Innovations in industrial practices are key  to achieving \n",
      "sustainability goals.  \n",
      "Reforestation and Afforestation...\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the greenhouse effect?\"\n",
    "results = retrieve_hierarchical(query, summary_store, detailed_store)\n",
    "\n",
    "# Print results\n",
    "for chunk in results:\n",
    "    print(f\"Page: {chunk.metadata['page']}\")\n",
    "    print(f\"Content: {chunk.page_content}...\")  # Print first 100 characters\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
